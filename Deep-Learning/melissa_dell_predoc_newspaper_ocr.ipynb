{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Convert Images to Text\n",
    "\n",
    "Most historical documents (e.g., newspapers) are only available to researchers in the form of image scans. The first step in analyzing the content of a historical document is therefore to perform image-to-text conversion. This is a combination of Layout Analysis and Optical Character Recognition (OCR), whereby you parse the arrangement of \"layout objects\" (e.g., headlines, author bylines) in documents and turn the image data inside each layout object region into a computable text. Luckily, a team member has just finished the layout analysis for you! With the given *layout information* and *images* as inputs, you need to generate structured data in the form of full newspaper articles. Structuring the OCRed text data as newspaper articles using layout information is a prerequisite for performing further analysis on the content of historical document scans.\n",
    "\n",
    "**Input**:\n",
    "\n",
    "- A list of images and layout information: `List[Tuple(image_path, layout_info)]`\n",
    "- The image and layout information files are stored in the `dataset/images` folder and `dataset/layout.json` file\n",
    "- The `layout.json` will be stored in a format like the [COCO format](https://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch): \n",
    "    \n",
    "    ```\n",
    "    {\n",
    "        'annotations': [\n",
    "            # Specifies the rectangular boxes and types for \n",
    "            # each layout element \n",
    "            {\"id\": 1,        # The id of the anno object\n",
    "             \"bbox\": [],     # (x_top_left, y_top_left, width, height) of the bounding box\n",
    "             \"image_id\": 1,  # The associated image_id\n",
    "             \"category_id\":2 # The category_id \n",
    "            }, ...\n",
    "        ],\n",
    "        'images': [\n",
    "            {\"id\": 397133,\n",
    "             \"height\": 427,\n",
    "             \"width\": 640,\n",
    "             \"file_name\": \"000000397133.jpg\"\n",
    "            }, ...\n",
    "        ],\n",
    "        'categories': [\n",
    "            {\"supercategory\": None, # We don't use this\n",
    "             \"id\": 1,               # The category_id used for each annotation\n",
    "             \"name\": \"person\"       # The name for the categories \n",
    "             }, ...\n",
    "        ],\n",
    "        'reading_orders': [\n",
    "            # Specifies the reading order of different annotation objects \n",
    "            # The object of `end_id` comes after the object of `start_id`\n",
    "            # Not all objects have the reading orders \n",
    "            {\n",
    "              \"from_id\": 435,\n",
    "              \"to_id\": 434,\n",
    "              \"image_id\": 9\n",
    "            }, ...\n",
    "        ]\n",
    "    }\n",
    "    ```\n",
    "\n",
    "\n",
    "**Output**:\n",
    "    \n",
    "- A list of parsed article information: `List[Dict(article_info)]`\n",
    "    ```\n",
    "    [\n",
    "        {'id': 0,            # An articled id generated by you (better be continuous from 0 to len(articles))\n",
    "         'headline': '',     # The headline of an article. Could be empty \n",
    "         'text': '',         # The body text of the article\n",
    "         'image_id': 12,     # The id of the image where the article appears \n",
    "         'anno_ids': [31,33] # The ids of the associated layout regions \n",
    "        }, ...\n",
    "    ]\n",
    "    ```\n",
    "    \n",
    "**Steps**:\n",
    "\n",
    "1. Iterate through the layout information.\n",
    "2. Using the layout info, perform OCR on layout object regions with the open source tool TesseractOCR.\n",
    "3. Join the layout object OCR texts using the reading orders available in the layout info.\n",
    "4. Combine all the outputs and save in the aforementioned format.\n",
    "\n",
    "**Notes**:\n",
    "\n",
    "- You might need to install the [TesseractOCR engine](https://github.com/tesseract-ocr/tesseract#installing-tesseract) and its [Python wrapper](https://github.com/madmaze/pytesseract) on your computer to perform the OCR.\n",
    "- You might find some functions in the [Layout Parser Library](https://layout-parser.readthedocs.io/en/latest/) helpful.\n",
    "- For the reading orders:\n",
    "    - The below image (or the `reading-order.png` file in the same folder as the Jupyter notebook document) shows an example of the layout objects in a full article and their reading order. \n",
    "    - A full article is usually composed of several text regions of different types. They are usually associated by several one-directional reading order pointers, illustrated by red arrows in the figure. The `from_id` is the annotation (object) id for the starting box, and the `to_id` is for the ending box. \n",
    "    - For the end of the article, there **won't** be an extra reading order point to the next title. So the reading order sequence within a full article is terminated at the last article box. \n",
    "    - Sometimes we will have reading orders for images and their captions. You can ignore these reading orders. \n",
    "    ![reading-order](reading-order.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $ pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe' # Please change the PATH here\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "\n",
    "def load_json(filename: str) -> Dict:\n",
    "    with open(filename, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def ocr_image_region(image: Image, bbox: Tuple[int, int, int, int]) -> str:\n",
    "    region = image.crop((bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]))\n",
    "    return pytesseract.image_to_string(region, lang='eng')\n",
    "\n",
    "def clean_ocr_text(text: str) -> str:\n",
    "    text = text.replace('‘', \"'\").replace('’', \"'\").replace('“', '\"').replace('”', '\"')\n",
    "    text = re.sub(r'(\\w)-\\n(\\w)', r'\\1\\2', text) # Remove hyphens between words at the end of the line\n",
    "    text = re.sub(r'(?<=[.!?;:])\\n(?=\\w)', ' ', text)\n",
    "    text = re.sub(r'(?<=[a-zA-Z0-9,])\\n(?=[a-zA-Z0-9])', ' ', text)\n",
    "    text = re.sub(r'\\n+', ' ', text)\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    text = re.sub(r'\\b(\\w{2,}),\\s*([A-Z])', r'\\1. \\2', text.strip(\",\")) # Some \".\" are mistaken as \".\" by ocr\n",
    "    text = re.sub(r'(\\d),(?=\\d{3}\\b)', r'\\1', text)\n",
    "    text = text.strip()\n",
    "    text = text[:-1] + \".\" if text and text.endswith(\",\") else text\n",
    "    return text\n",
    "\n",
    "def parse_newspaper_scan(images_info: List[Dict], annotations: List[Dict], categories: List[Dict], reading_orders: List[Dict]) -> List[Dict]:\n",
    "    category_map = {category['id']: category['name'] for category in categories} \n",
    "    \n",
    "    image_annotations = {image['id']: [] for image in images_info}\n",
    "    for annotation in annotations:\n",
    "        image_annotations[annotation['image_id']].append(annotation)\n",
    "    \n",
    "    articles = []\n",
    "    article_id = 0\n",
    "    \n",
    "    for image_info in images_info:\n",
    "        image_path = image_info['file_name']\n",
    "        image_id = image_info['id']\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        current_annotations = image_annotations[image_id]\n",
    "        current_reading_orders = [ro for ro in reading_orders if ro['image_id'] == image_id]\n",
    "        \n",
    "        ro_map = {ro['from_id']: ro['to_id'] for ro in current_reading_orders}\n",
    "        \n",
    "        all_to_ids = {ro['to_id'] for ro in current_reading_orders}\n",
    "        start_ids = [ann['id'] for ann in current_annotations if ann['id'] not in all_to_ids] # Find the starting id\n",
    "        \n",
    "        for start_id in start_ids:\n",
    "            current_article = {\n",
    "                'id': article_id,\n",
    "                'headline': '',\n",
    "                'text': '',\n",
    "                'image_id': image_id,\n",
    "                'anno_ids': []\n",
    "            }\n",
    "            \n",
    "            # Iterate through the layout information\n",
    "            current_id = start_id\n",
    "            while current_id:\n",
    "                annotation = next((ann for ann in current_annotations if ann['id'] == current_id), None)\n",
    "                if annotation:\n",
    "                    category_name = category_map[annotation['category_id']]\n",
    "                    text = ocr_image_region(image, annotation['bbox'])\n",
    "                    cleaned_text = clean_ocr_text(text)\n",
    "                    \n",
    "                    if category_name == 'headline':\n",
    "                        current_article['headline'] += cleaned_text + \" \"\n",
    "                    else:\n",
    "                        current_article['text'] += cleaned_text + \" \"\n",
    "                    \n",
    "                    current_article['anno_ids'].append(current_id)\n",
    "                \n",
    "                current_id = ro_map.get(current_id, None)\n",
    "                \n",
    "            current_article['headline'] = current_article['headline'].strip()\n",
    "            current_article['text'] = current_article['text'].strip()\n",
    "            \n",
    "            articles.append(current_article)\n",
    "            article_id += 1\n",
    "    \n",
    "    return articles\n",
    "\n",
    "# Read the json and perform OCR\n",
    "file_path = os.path.join(parent_dir, \"dataset\",\"layout.json\")\n",
    "with open(file_path, 'r') as f:\n",
    "    layout_data = json.load(f)\n",
    "images_info = layout_data['images']\n",
    "annotations = layout_data['annotations']\n",
    "categories = layout_data['categories']\n",
    "reading_orders = layout_data['reading_orders']\n",
    "\n",
    "for image in images_info:\n",
    "    image['file_name'] = os.path.join(parent_dir,\"dataset\",\"images\",image[\"file_name\"])\n",
    "    \n",
    "parsed_articles = parse_newspaper_scan(images_info, annotations, categories, reading_orders) # Perform OCR\n",
    "\n",
    "\"\"\"print(json.dumps(parsed_articles, indent=2, ensure_ascii=False))\"\"\"\n",
    "\n",
    "output_file_path = os.path.join(parent_dir,\"results\",\"parsed_articles.json\")\n",
    "with open(output_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "    json.dump(parsed_articles, output_file, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"JSON data saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Run SBERT on the Text \n",
    "\n",
    "After parsing the articles from the images, we want to use natural language processing (NLP) methods to analyze the text. As texts are usually heterogeneous, we use novel deep learning models like [Sentence-BERT (SBERT)](https://arxiv.org/abs/1908.10084) to convert them into dense numerical representations (i.e., vectors) to enable meaningful quantitative analysis. Fortunately, SBERT implementations and pre-trained model weights are [open source](https://sbert.net/). Thus, we don't need to spend a huge amount of time implementing the models of interest to us and training them from scratch. \n",
    "\n",
    "**Input**: \n",
    "- The list of articles from Part 1\n",
    "\n",
    "**Output**:\n",
    "- An $M\\times N$ matrix where each row is the embedding (dense numerical representation) vector for that article \n",
    "    - $M$: len(articles)\n",
    "    - $N$: The embedding dimension. \n",
    "    \n",
    "**Steps**:\n",
    "1. Build the SBERT model and load the weights from the pretrained model `all-MiniLM-L6-v2`. \n",
    "2. For each article, use SBERT to \"embed\" (create a dense numerical representation of) the text. \n",
    "    - You can refer to this official documentation for creating the sentence embeddings using SBERT: https://sbert.net/index.html\n",
    "    - Questions to consider:\n",
    "        - How do you deal with the noise (e.g., spelling mistakes, punctuation errors) in the OCRed text? Is this noise a problem for SBERT?\n",
    "        - How do you handle embedding very long articles?  \n",
    "\n",
    "**Notes**:\n",
    "- You will need to install and learn to use the official [sentence-transformer library](https://sbert.net/docs/installation.html) and their SBERT implementation and pre-trained weights.\n",
    "- You can consider generating embedding vectors for just the headlines of each article, which are usually short sentences and may prove more manageable to work with. If you do so, provide a brief discussion of the tradeoffs involved in this decision in Part 4, and whether or not you would want to do this in practice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# Using Rapidfuzz to correct spelling, for comparison\n",
    "\"\"\"\n",
    "from rapidfuzz import process, fuzz\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "dictionary = words.words()\n",
    "\"\"\"\n",
    "\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "\n",
    "file_path = os.path.join(parent_dir, \"results\",\"parsed_articles.json\")\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    articles = json.load(file)\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2') # SBERT model\n",
    "LENGTH_THRESHOLD = 512 # This can be tuned\n",
    "    \n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r\"([a-zA-Z])(?=\\s+[A-Z])\", r\"\\1. \", text) # Deal with punctuation issues\n",
    "    text = re.sub(r'[^\\w\\s,.]', '', text)\n",
    "    # Using Rapidfuzz to correct spelling, for comparison\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    corrected_words = []\n",
    "    for word in words:\n",
    "        best_match = process.extractOne(word, dictionary, scorer=fuzz.WRatio, score_cutoff=70)\n",
    "        if best_match:\n",
    "            corrected_words.append(best_match[0])\n",
    "        else:\n",
    "            corrected_words.append(word)\n",
    "    text = ' '.join(corrected_words)\n",
    "    \"\"\"\n",
    "    return text\n",
    "\n",
    "def embed_short_article(article):\n",
    "    return model.encode(article)\n",
    "\n",
    "def embed_long_article(article, max_chunk_length=256):\n",
    "    \"\"\"Split the article into chunks\"\"\"\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', article)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_length = len(sentence.split())\n",
    "        if current_length + sentence_length > max_chunk_length:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_length = sentence_length\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += sentence_length\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "        \n",
    "    chunk_embeddings = np.array([model.encode(chunk) for chunk in chunks])\n",
    "    article_embedding = np.mean(chunk_embeddings, axis=0) # Embed each chunk and then average\n",
    "    \n",
    "    return article_embedding\n",
    "\n",
    "def automatic_embed_article(article, length_threshold=LENGTH_THRESHOLD):\n",
    "    tokens = article.split()\n",
    "    num_tokens = len(tokens)\n",
    "    if num_tokens <= length_threshold:\n",
    "        return embed_short_article(article)\n",
    "    else:\n",
    "        return embed_long_article(article)\n",
    "    \n",
    "def embed_newspaper_texts(articles, output_file_path):\n",
    "    embeddings = []\n",
    "    for article in articles:\n",
    "        if 'text' in article and article['text'].strip() != '':\n",
    "            text = preprocess_text(article['text'])\n",
    "            embedding = automatic_embed_article(text)\n",
    "            embeddings.append(embedding)\n",
    "            \n",
    "    embeddings = np.array(embeddings)\n",
    "    \n",
    "    print(\"Shape of embeddings:\", embeddings.shape)\n",
    "    \"\"\"print(\"Embeddings Matrix (first 2 rows):\\n\", embeddings[:2])\"\"\"\n",
    "    np.savetxt(output_file_path, embeddings, delimiter=\",\")\n",
    "        \n",
    "    return embeddings\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    output_file_path = os.path.join(parent_dir,\"results\",\"article_embeddings.csv\")\n",
    "    embeddings = embed_newspaper_texts(articles, output_file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## COMPARE TWO EMBEDDINGS (1- with noise; 2- correct spelling)\n",
    "\n",
    "\"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "\n",
    "def evaluate_embeddings(embeddings1, embeddings2):\n",
    "    similarities = []\n",
    "    for emb1, emb2 in zip(embeddings1, embeddings2):\n",
    "        similarity = 1 - cosine(emb1, emb2) # Average cosine similarity\n",
    "        similarities.append(similarity)\n",
    "    return np.mean(similarities)\n",
    "\n",
    "csv_path1 = os.path.join(parent_dir,\"results\",\"article_embeddings.csv\")\n",
    "csv_path2 = os.path.join(parent_dir,\"results\",\"article_embeddings_robust.csv\")\n",
    "\n",
    "embeddings1 = pd.read_csv(csv_path1, header=None).values\n",
    "embeddings2 = pd.read_csv(csv_path2, header=None).values\n",
    "\n",
    "embeddings1 = np.array(embeddings1)\n",
    "embeddings2 = np.array(embeddings2)\n",
    "\n",
    "average_similarity = evaluate_embeddings(embeddings1, embeddings2)\n",
    "\n",
    "print(\"Average Cosine Similarity:\", average_similarity)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Visualization and Analysis \n",
    "\n",
    "It's hard for humans to directly interpret these dense, high-dimensional embedding vectors. Therefore, to make the results of SBERT inference more transparent and intelligible, it is common to (a) perform dimensionality reduction on and (b) cluster the embedding vectors. In this part, you are required to analyze the vectors you generated in a similar way. \n",
    "\n",
    "**Input**:\n",
    "- The embedding matrix/table from Part 2\n",
    "\n",
    "**Output**:\n",
    "- Visualization of the embedding vectors in a low-dimension space \n",
    "- Clustering results of the embedding vectors \n",
    "- Interpretation of the results, including but not limited to: \n",
    "    - Do the clusters of vectors look meaningful? \n",
    "    - Could you find clusters of important topics? \n",
    "\n",
    "**Steps**:\n",
    "1. Choose an appropriate dimensionality reduction method and apply it to the embedding matrix. \n",
    "2. Choose an appropriate clustering method and apply it to the embedding matrix or the embedding matrix after dimensionality reduction.\n",
    "3. Visualize the results using the tools that are most familiar to you and save the results.\n",
    "\n",
    "**Notes**:\n",
    "- Scikit-learn provides many handy functions for [clustering](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.cluster) and dimensionality reduction (e.g., [matrix decomposition](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition), [manifold learning methods such as t-SNE](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.manifold)).\n",
    "- You can visualize the results in 2D or 3D. For drawing 3D plots, you can consider using [plotly](https://plotly.com/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### I DID BOTH FOR 2- AND 3-DIMENSION.\n",
    "#### THIS IS FOR 2-DIMENSION\n",
    "\n",
    "import os\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.cluster._kmeans\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn.cluster._kmeans\")\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "print(\"OMP_NUM_THREADS:\", os.getenv(\"OMP_NUM_THREADS\")) # Confirm the environment being set\n",
    "\n",
    "import numpy as np\n",
    "from threadpoolctl import threadpool_limits\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import plot as plotly_plot\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "import string\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "custom_stopwords = set([\"said\",\"would\",\"year\",\"today\",\"one\",\"Mr\",\"Mrs\",\"Dr\"])\n",
    "custom_stopwords.update(string.ascii_uppercase)\n",
    "all_stopwords = stop_words.union(custom_stopwords)\n",
    "\n",
    "output_dir = os.path.join(parent_dir, 'results', 'plot')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def clustering_and_viz(embs):\n",
    "    with threadpool_limits(limits=1, user_api='blas'):\n",
    "        print(\"Thread limits set within threadpool_limits context.\")\n",
    "        \n",
    "        tsne = TSNE(n_components=2, random_state=42) # Dimensionality Reduction\n",
    "        embeddings_2d = tsne.fit_transform(embs)\n",
    "        \n",
    "        n_clusters = 5\n",
    "        kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "        clusters = kmeans.fit_predict(embeddings_2d)\n",
    "        \n",
    "        # Visualization\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for i in range(n_clusters):\n",
    "            plt.scatter(embeddings_2d[clusters == i, 0], embeddings_2d[clusters == i, 1], label=f\"Cluster {i}\")\n",
    "        plt.title('2D visualization of article embeddings with t-SNE')\n",
    "        plt.legend()\n",
    "        plot_path = os.path.join(output_dir, 'scatter_plot_2D_2comps.png')\n",
    "        plt.savefig(plot_path)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        html_content = \"<html><head><title>Clustering Results</title>\"\n",
    "        html_content += \"<script src='https://cdn.plot.ly/plotly-latest.min.js'></script></head><body>\"\n",
    "        html_content += \"<h1>2D Visualization</h1>\"\n",
    "        html_content += f\"<img src='plot/scatter_plot_2D_2comps.png' /><br>\"\n",
    "        html_content += \"<h1>Word Clouds for Clusters</h1>\"\n",
    "    \n",
    "        # Cluster topics\n",
    "        for i in range(n_clusters):\n",
    "            cluster_indices = np.where(clusters == i)[0]\n",
    "            cluster_articles = articles_array[cluster_indices]\n",
    "            print(f\"\\nCluster {i} - {len(cluster_articles)} articles:\")\n",
    "\n",
    "            # Wordclouds for clusters\n",
    "            cluster_text = ' '.join(cluster_articles.flatten())\n",
    "            wordcloud = WordCloud(width=800, height=400, background_color='white', stopwords=all_stopwords).generate(cluster_text)\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.imshow(wordcloud, interpolation='bilinear')\n",
    "            plt.title(f'Word Cloud for Cluster {i}')\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "            wordcloud_path = os.path.join(output_dir, f'wordcloud_cluster_{i}_2comps.png')\n",
    "            wordcloud.to_file(wordcloud_path)\n",
    "\n",
    "            html_content += f\"<h2>Cluster {i}</h2><img src='{wordcloud_path}' /><br>\"\n",
    "        html_content += \"</body></html>\"\n",
    "        html_path = os.path.join(parent_dir,\"results\",\"visualization_results_2D.html\")\n",
    "        with open(html_path, \"w\") as html_file:\n",
    "            html_file.write(html_content)\n",
    "            \n",
    "# Perform clustering and visualization\n",
    "file_path_embeddings = os.path.join(parent_dir,\"results\",\"article_embeddings.csv\")\n",
    "file_path_articles = os.path.join(parent_dir,\"results\",\"parsed_articles.json\")\n",
    "\n",
    "embeddings = pd.read_csv(file_path_embeddings, header=None).values\n",
    "\n",
    "with open(file_path_articles, 'r', encoding='utf-8') as f:\n",
    "    articles_json = json.load(f)\n",
    "article_texts = [article['text'] for article in articles_json]\n",
    "articles_array = np.array(article_texts)\n",
    "\n",
    "clustering_and_viz(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### THIS IS FOR 3-DIMENSION\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from wordcloud import WordCloud\n",
    "import json\n",
    "import string\n",
    "\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "custom_stopwords = set([\"said\",\"would\",\"year\",\"today\",\"one\",\"Mr\",\"Mrs\",\"Dr\"])\n",
    "custom_stopwords.update(string.ascii_uppercase)\n",
    "all_stopwords = stop_words.union(custom_stopwords)\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\" \n",
    "\n",
    "output_dir = os.path.join(parent_dir, 'results', 'plot')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "def clustering_and_viz_2d_3d(embs):\n",
    "    tsne = TSNE(n_components=2, random_state=42) # Dimensionality Reduction to 2D using t-SNE\n",
    "    embeddings_2d = tsne.fit_transform(embs)\n",
    "    \n",
    "    pca = PCA(n_components=3) # Dimensionality Reduction to 3D using PCA\n",
    "    embeddings_3d = pca.fit_transform(embs)\n",
    "    \n",
    "    n_clusters = 5\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "    clusters = kmeans.fit_predict(embs)\n",
    "    \n",
    "    # 2D Visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=clusters, cmap='viridis')\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title('2D Visualization of Article Embeddings with t-SNE and K-Means Clustering')\n",
    "    plt.xlabel('Component 1')\n",
    "    plt.ylabel('Component 2')\n",
    "    plot_path = os.path.join(output_dir, 'scatter_plot_2D.png')\n",
    "    plt.savefig(plot_path)\n",
    "    plt.show()\n",
    "     \n",
    "    # 3D Visualization\n",
    "    trace = go.Scatter3d(\n",
    "        x=embeddings_3d[:, 0],\n",
    "        y=embeddings_3d[:, 1],\n",
    "        z=embeddings_3d[:, 2],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=5,\n",
    "            color=clusters,\n",
    "            colorscale='Viridis',\n",
    "            opacity=0.8,\n",
    "            showscale=True\n",
    "        )\n",
    "    )\n",
    "    plotly_fig = go.Figure([trace])\n",
    "    plotly_html_3D = plotly_plot(plotly_fig, output_type='div', include_plotlyjs=False)\n",
    "    iplot(plotly_fig)\n",
    "\n",
    "    html_content = \"<html><head><title>Clustering Results</title>\"\n",
    "    html_content += \"<script src='https://cdn.plot.ly/plotly-latest.min.js'></script></head><body>\"\n",
    "    html_content += \"<h1>2D Visualization</h1>\"\n",
    "    html_content += f\"<img src='plot/scatter_plot_2D.png' /><br>\"\n",
    "    html_content += \"<h1>3D Visualization</h1>\"\n",
    "    html_content += f\"{plotly_html_3D}\"\n",
    "    html_content += \"<h1>Word Clouds for Clusters</h1>\"\n",
    "    \n",
    "    \n",
    "    # Cluster topics\n",
    "    for i in range(n_clusters):\n",
    "        cluster_indices = np.where(clusters == i)[0]\n",
    "        cluster_articles = articles_array[cluster_indices]\n",
    "        print(f\"\\nCluster {i} - {len(cluster_articles)} articles:\")\n",
    "            \n",
    "        # Wordclouds for clusters\n",
    "        cluster_text = ' '.join(cluster_articles.flatten())\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white', stopwords=all_stopwords).generate(cluster_text)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.title(f'Word Cloud for Cluster {i}')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        wordcloud_path = os.path.join(output_dir, f'wordcloud_cluster_{i}.png')\n",
    "        wordcloud.to_file(wordcloud_path)\n",
    "        html_content += f\"<h2>Cluster {i}</h2><img src='{wordcloud_path}' /><br>\"\n",
    "    html_content += \"</body></html>\"\n",
    "    html_path = os.path.join(parent_dir,\"results\",\"visualization_results_3D.html\")\n",
    "    with open(html_path, \"w\") as html_file:\n",
    "        html_file.write(html_content)\n",
    "            \n",
    "            \n",
    "# Perform clustering and visualization\n",
    "file_path_embeddings = os.path.join(parent_dir,\"results\",\"article_embeddings.csv\")\n",
    "file_path_articles = os.path.join(parent_dir,\"results\",\"parsed_articles.json\")\n",
    "\n",
    "embeddings = pd.read_csv(file_path_embeddings, header=None).values\n",
    "with open(file_path_articles, 'r', encoding='utf-8') as f:\n",
    "    articles_json = json.load(f)\n",
    "article_texts = [article['text'] for article in articles_json]\n",
    "articles_array = np.array(article_texts)\n",
    "\n",
    "clustering_and_viz_2d_3d(embeddings)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
